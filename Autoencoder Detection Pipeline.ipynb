{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Load Datasets"
      ],
      "metadata": {
        "id": "yxRkWXZG6dNq"
      },
      "id": "yxRkWXZG6dNq"
    },
    {
      "cell_type": "code",
      "id": "jU40cH37vfQvMAW4ntBw749Y",
      "metadata": {
        "tags": [],
        "id": "jU40cH37vfQvMAW4ntBw749Y"
      },
      "source": [
        "# All benign and attack files used in this experiment can be accessed here: https://www.kaggle.com/datasets/drvoyager/model-probing-attack-dataset\n",
        "\n",
        "# Pick one: \"cifar\" or \"imagenet\"\n",
        "DATASET = \"cifar\"\n",
        "\n",
        "# Paste your local file paths (edit these strings)\n",
        "PATHS = {\n",
        "    \"cifar\": {\n",
        "        \"benign\":   r\"/path/to/CIFAR-10/Benign.csv\",\n",
        "        \"hsja\":     r\"/path/to/CIFAR-10/HSJA.csv\",\n",
        "        \"nes\":      r\"/path/to/CIFAR-10/NES.csv\",\n",
        "        \"qeba\":     r\"/path/to/CIFAR-10/QEBA.csv\",\n",
        "        \"square\":   r\"/path/to/CIFAR-10/Square.csv\",\n",
        "        \"surfree\":  r\"/path/to/CIFAR-10/SURFREE.csv\",\n",
        "        \"boundary\": r\"/path/to/CIFAR-10/Boundary.csv\",\n",
        "    },\n",
        "    \"imagenet\": {\n",
        "        \"benign\":    r\"/path/to/ImageNet/Benign.csv\",\n",
        "        \"hsja\":      r\"/path/to/ImageNet/HSJA.csv\",\n",
        "        \"nes\":       r\"/path/to/ImageNet/NES.csv\",\n",
        "        \"qeba\":      r\"/path/to/ImageNet/QEBA.csv\",\n",
        "        \"square\":    r\"/path/to/ImageNet/Square.csv\",\n",
        "        \"surfree\":   r\"/path/to/ImageNet/SURFREE.csv\",\n",
        "        \"boundary\":  r\"/path/to/ImageNet/Boundary.csv\",\n",
        "        \"synthetic\": r\"/path/to/Synthetic_Data.csv\",\n",
        "    },\n",
        "}\n",
        "\n",
        "# Only these columns are needed everywhere\n",
        "USE_COLS = [\"SortedList_First_Element\", \"SortedList_Second_Element\", \"Margin Loss\"]\n",
        "\n",
        "# Attacks are capped at 14k for a fair evalutation\n",
        "_NO_CAP = {\"benign\", \"synthetic\"}\n",
        "_CAP_ROWS = 14_000\n",
        "\n",
        "def load(name: str):\n",
        "    \"\"\"Read a CSV locally using PATHS and return a pandas DataFrame.\"\"\"\n",
        "    df = pd.read_csv(PATHS[DATASET][name], usecols=USE_COLS)\n",
        "    return df if name in _NO_CAP else df.iloc[:_CAP_ROWS]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "hTjHgIRO9puT"
      },
      "id": "hTjHgIRO9puT"
    },
    {
      "cell_type": "code",
      "source": [
        "if DATASET.lower() == \"imagenet\":\n",
        "    # ImageNet: train on synthetic benign; validate/test on real benign\n",
        "    benign_df    = load(\"benign\")      # real benign (local CSV)\n",
        "    synthetic_df = load(\"synthetic\")   # synthetic benign (local CSV)\n",
        "\n",
        "    # Split real benign into 60% val / 40% test\n",
        "    b_val, b_test = train_test_split(benign_df, test_size=0.4, random_state=42)\n",
        "\n",
        "    # Training = all synthetic (shuffled, index reset for clean slicing)\n",
        "    train_df = synthetic_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    val_df   = b_val.reset_index(drop=True)\n",
        "    test_df  = b_test.reset_index(drop=True)\n",
        "else:\n",
        "    # CIFAR: 60/20/20 split on real benign (train/val/test)\n",
        "    real_df = load(\"benign\")\n",
        "\n",
        "    # 60% train / 40% temp\n",
        "    train_df, temp_df = train_test_split(real_df, test_size=0.4, random_state=42)\n",
        "    # temp -> 50/50 => 20% val / 20% test overall\n",
        "    val_df, test_df   = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Clean indices for downstream numpy slicing\n",
        "    train_df = train_df.reset_index(drop=True)\n",
        "    val_df   = val_df.reset_index(drop=True)\n",
        "    test_df  = test_df.reset_index(drop=True)\n",
        "\n",
        "# Feature extraction for the AE\n",
        "def extract_features_for_autoencoder(sequence_data, window_size=600, stride=60, chunk_size=100):\n",
        "    top    = np.array(sequence_data['SortedList_First_Element'])\n",
        "    second = np.array(sequence_data['SortedList_Second_Element'])\n",
        "    margin = np.array(sequence_data['Margin Loss'])\n",
        "\n",
        "    n_chunks = window_size // chunk_size\n",
        "    seqs = []\n",
        "\n",
        "    # Slide a window of length `window_size` with step `stride`\n",
        "    for i in range(0, len(top) - window_size + 1, stride):\n",
        "        w1 = top[i:i+window_size]\n",
        "        w2 = second[i:i+window_size]\n",
        "        w3 = margin[i:i+window_size]\n",
        "\n",
        "        # Per-chunk stats over the margin signal\n",
        "        mean_margin_chunks, var_margin_chunks = [], []\n",
        "        for j in range(n_chunks):\n",
        "            s, e = j * chunk_size, (j + 1) * chunk_size\n",
        "            mm, vm = np.mean(w3[s:e]), np.var(w3[s:e])\n",
        "            # Broadcast each chunk's stat across the whole window\n",
        "            mean_margin_chunks.append(np.full(window_size, mm))\n",
        "            var_margin_chunks.append(np.full(window_size, vm))\n",
        "\n",
        "        # Stack base signals + chunk means + chunk variances -> (window_size, features)\n",
        "        features = [w1, w2, w3] + mean_margin_chunks + var_margin_chunks\n",
        "        seqs.append(np.column_stack(features))\n",
        "\n",
        "    return np.array(seqs)\n",
        "\n",
        "# ----- Build AE-ready tensors -----\n",
        "train_sequences = extract_features_for_autoencoder(train_df)\n",
        "val_sequences   = extract_features_for_autoencoder(val_df)\n",
        "test_sequences  = extract_features_for_autoencoder(test_df)\n",
        "\n",
        "print(f\"Train: {train_sequences.shape}\")\n",
        "print(f\"Val:   {val_sequences.shape}\")\n",
        "print(f\"Test:  {test_sequences.shape}\")\n"
      ],
      "metadata": {
        "id": "IIoZwfGYph2G"
      },
      "execution_count": null,
      "outputs": [],
      "id": "IIoZwfGYph2G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autoencoder Architecture & Training"
      ],
      "metadata": {
        "id": "jFNPPB87q4S9"
      },
      "id": "jFNPPB87q4S9"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_autoencoder(input_shape):\n",
        "    # Input\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    x = Conv1D(64, kernel_size=5, padding='same', activation='relu')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "    x = Conv1D(32, kernel_size=7, padding='same', activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "    x = Conv1D(16, kernel_size=9, padding='same', activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "    # Bottleneck\n",
        "    x = Conv1D(8, kernel_size=11, padding='same', activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # Decoder\n",
        "    x = Conv1DTranspose(16, kernel_size=9, strides=2, padding='same', activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = Conv1DTranspose(32, kernel_size=7, strides=2, padding='same', activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = Conv1DTranspose(64, kernel_size=5, strides=2, padding='same', activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # Output\n",
        "    outputs = Conv1D(input_shape[1], kernel_size=3, padding='same', activation='linear')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-3), loss='mse')\n",
        "    return model\n",
        "\n",
        "# Define input shape from precomputed sequences\n",
        "input_shape = (train_sequences.shape[1], train_sequences.shape[2])\n",
        "print(f\"Input shape: {input_shape}\")\n",
        "\n",
        "# Build model\n",
        "autoencoder = build_autoencoder(input_shape)\n",
        "autoencoder.summary()\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-4)\n",
        "\n",
        "# Train\n",
        "print(\"Training autoencoder...\")\n",
        "history = autoencoder.fit(\n",
        "    train_sequences, train_sequences,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_data=(val_sequences, val_sequences),\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(8, 5), dpi=300)\n",
        "plt.plot(history.history['loss'], label='Training Loss', linewidth=2.5)\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2.5)\n",
        "plt.title(f'{DATASET.upper()} AE Training History', fontsize=14, weight='bold')\n",
        "plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.legend(fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save model\n",
        "autoencoder.save(f\"results/{DATASET.upper()}_AE.h5\")\n",
        "\n"
      ],
      "metadata": {
        "id": "KTSj-ZIZq8cI"
      },
      "execution_count": null,
      "outputs": [],
      "id": "KTSj-ZIZq8cI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results and Analysis"
      ],
      "metadata": {
        "id": "qeduAV0h9fkP"
      },
      "id": "qeduAV0h9fkP"
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_reconstruction_errors(original, reconstructed):\n",
        "    # Per-sequence errors across time (axis=1) and features (axis=2)\n",
        "    mse = np.mean((original - reconstructed) ** 2, axis=(1, 2))\n",
        "    mae = np.mean(np.abs(original - reconstructed), axis=(1, 2))\n",
        "    return {\"mse\": mse, \"mae\": mae}\n",
        "\n",
        "# 1) Fit thresholds on validation set (mean + 2.5*std per metric)\n",
        "val_recon  = autoencoder.predict(val_sequences, verbose=0)\n",
        "val_errors = calculate_reconstruction_errors(val_sequences, val_recon)\n",
        "thresholds = {m: np.mean(v) + 2.5 * np.std(v) for m, v in val_errors.items()}\n",
        "print(\"Thresholds:\", {k: round(v, 6) for k, v in thresholds.items()})\n",
        "\n",
        "# 2) Benign test errors (for comparison in histograms)\n",
        "test_reconstructed = autoencoder.predict(test_sequences, verbose=0)\n",
        "test_errors = calculate_reconstruction_errors(test_sequences, test_reconstructed)\n",
        "\n",
        "# 3) Evaluate chosen attacks\n",
        "for atk in [\"nes\", \"square\", \"hsja\", \"qeba\", \"boundary\", \"surfree\"]: # Can add or remove black-box attacks\n",
        "    # Load attack CSV locally and make AE-ready sequences\n",
        "    df    = load(atk)\n",
        "    seqs  = extract_features_for_autoencoder(df)\n",
        "\n",
        "    # AE recon + errors for the attack sequences\n",
        "    recon = autoencoder.predict(seqs, verbose=0)\n",
        "    errs  = calculate_reconstruction_errors(seqs, recon)\n",
        "\n",
        "    # Binary flags per metric using thresholds\n",
        "    flags = {m: errs[m] > thresholds[m] for m in errs}\n",
        "\n",
        "    # Combine metrics: flag if either MSE or MAE is above threshold\n",
        "    combined = flags[\"mse\"] | flags[\"mae\"]\n",
        "\n",
        "    # Labels: benign test (0) vs attack (1)\n",
        "    y_true = np.concatenate([np.zeros(len(test_sequences)), np.ones(len(combined))])\n",
        "    y_pred = np.concatenate([np.zeros(len(test_sequences)), combined.astype(int)])\n",
        "\n",
        "    print(f\"\\n=== {atk.upper()} Evaluation ===\")\n",
        "    print(classification_report(y_true, y_pred, target_names=[\"Benign\", \"Attack\"], digits=2))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d',\n",
        "                xticklabels=[\"Benign\", \"Attack\"], yticklabels=[\"Benign\", \"Attack\"])\n",
        "    plt.title(f\"{atk.upper()} CM ({DATASET})\")\n",
        "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "    # Histograms: compare attack errors vs benign test errors\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    for i, m in enumerate([\"mse\", \"mae\"], 1):\n",
        "        plt.subplot(1, 2, i)\n",
        "        sns.histplot(errs[m], bins=50, kde=True, stat='count', label=f'{atk.upper()}', color='red')\n",
        "        sns.histplot(test_errors[m], bins=50, kde=True, stat='count', label='Benign', color='blue')\n",
        "        plt.axvline(thresholds[m], linestyle='--', color='black',\n",
        "                    label=f\"{m.upper()} Threshold: {thresholds[m]:.3f}\")\n",
        "        plt.title(m.upper()); plt.legend()\n",
        "    plt.tight_layout(); plt.show()\n"
      ],
      "metadata": {
        "id": "yL2bsSyC8fqZ"
      },
      "id": "yL2bsSyC8fqZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adversarial Injection Test"
      ],
      "metadata": {
        "id": "g2lQGfB6AJRv"
      },
      "id": "g2lQGfB6AJRv"
    },
    {
      "cell_type": "code",
      "source": [
        "# benign stream helpers\n",
        "def shuffle_blocks(data, block_size=20):  # shuffle contiguous blocks to keep some local structure\n",
        "    blocks = [data[i:i+block_size] for i in range(0, len(data), block_size)]\n",
        "    random.shuffle(blocks)\n",
        "    return np.concatenate(blocks, axis=0)\n",
        "\n",
        "def randomly_drop_sequences(data, drop_prob=0.05):  # drop a few windows to add irregularity\n",
        "    keep_idx = [i for i in range(len(data)) if random.random() > drop_prob]\n",
        "    return data[keep_idx]\n",
        "\n",
        "# basic config for the injection test\n",
        "attack_list = [\"hsja\", \"qeba\", \"nes\", \"square\", \"surfree\", \"boundary\"]\n",
        "benign_seq_count = 2000           # total benign windows in the stream before injection\n",
        "num_attacks_to_inject = 10        # how many attack segments to insert\n",
        "\n",
        "# 1) build a benign stream from the held-out test set\n",
        "base_benign = extract_features_for_autoencoder(test_df)\n",
        "extended_benign = np.tile(base_benign, (benign_seq_count // len(base_benign) + 1, 1, 1))[:benign_seq_count]\n",
        "extended_benign = randomly_drop_sequences(shuffle_blocks(extended_benign), drop_prob=0.03)\n",
        "stream = extended_benign.copy()\n",
        "true_labels = np.zeros(len(stream), dtype=int)  # 0=benign, will flip to 1 over injected regions\n",
        "\n",
        "# 2) inject randomized attack segments with simple transformation strategies\n",
        "injected_regions, injected_attacks = [], []\n",
        "used_attacks, attempts = set(), 0\n",
        "\n",
        "def random_attack_len(low=5, high=600):  # choose attack length in AE windows\n",
        "    return random.randint(low, high)\n",
        "\n",
        "while len(injected_attacks) < num_attacks_to_inject and attempts < 30:\n",
        "    if len(used_attacks) == len(attack_list):\n",
        "        used_attacks.clear()\n",
        "\n",
        "    atk = random.choice([a for a in attack_list if a not in used_attacks])\n",
        "    used_attacks.add(atk)\n",
        "    seq_len = random_attack_len()\n",
        "    attempts += 1\n",
        "\n",
        "    try:\n",
        "        df = load(atk)  # read local CSV for this attack\n",
        "        attack_raw = extract_features_for_autoencoder(df)[:seq_len]\n",
        "        if len(attack_raw) == 0:\n",
        "            continue\n",
        "\n",
        "        strategy = random.choice([\"fragmented\", \"stretched\", \"ramped\", \"hybrid\", \"spike\"])\n",
        "\n",
        "        if strategy == \"fragmented\":         # keep every other window\n",
        "            attack_seqs = attack_raw[::2]\n",
        "\n",
        "        elif strategy == \"stretched\":        # duplicate windows to elongate\n",
        "            attack_seqs = np.repeat(attack_raw, 2, axis=0)\n",
        "\n",
        "        elif strategy == \"ramped\":           # gradually increase magnitude\n",
        "            ramped = attack_raw.copy()\n",
        "            for i in range(len(ramped)):\n",
        "                ramped[i] *= 1.0 + (i / max(1, len(ramped))) * 0.5\n",
        "            attack_seqs = ramped\n",
        "\n",
        "        elif strategy == \"hybrid\":           # half this attack + half another attack\n",
        "            alt_atk = random.choice([a for a in attack_list if a != atk])\n",
        "            alt_raw = extract_features_for_autoencoder(load(alt_atk))[:seq_len]\n",
        "            if len(alt_raw) == 0:\n",
        "                continue\n",
        "            half = len(attack_raw) // 2\n",
        "            attack_seqs = np.concatenate([attack_raw[:half], alt_raw[half:]], axis=0)\n",
        "\n",
        "        elif strategy == \"spike\":            # brief strong spike in the middle\n",
        "            spike = attack_raw[0:1] * 5\n",
        "            insert_at = len(attack_raw) // 2\n",
        "            attack_seqs = np.concatenate([attack_raw[:insert_at], spike, attack_raw[insert_at:]], axis=0)\n",
        "\n",
        "        if len(attack_seqs) == 0:\n",
        "            continue\n",
        "\n",
        "        # choose an insertion region near evenly spaced sections to reduce overlap\n",
        "        stream_len = len(stream)\n",
        "        section_size = stream_len // (num_attacks_to_inject + 2)\n",
        "        section_idx = len(injected_regions) + 1\n",
        "        start = max(0, section_idx * section_size - 40)\n",
        "        end = min(stream_len - len(attack_seqs) - 1, section_idx * section_size + 40)\n",
        "        if end <= start:\n",
        "            continue\n",
        "\n",
        "        insert_idx = random.randint(start, end)\n",
        "\n",
        "        # insert attack windows and mark labels as 1 in that span\n",
        "        stream = np.concatenate([stream[:insert_idx], attack_seqs, stream[insert_idx:]])\n",
        "        true_labels = np.concatenate([\n",
        "            true_labels[:insert_idx],\n",
        "            np.ones(len(attack_seqs), dtype=int),\n",
        "            true_labels[insert_idx:]\n",
        "        ])\n",
        "        injected_regions.append((insert_idx, insert_idx + len(attack_seqs)))\n",
        "        injected_attacks.append(f\"{atk.upper()}-{strategy} ({len(attack_seqs)})\")\n",
        "\n",
        "    except Exception:\n",
        "        continue  # skip if anything goes wrong with this attempt\n",
        "\n",
        "# 3) predict over validation to get thresholds (mean + 2.5*std), then over the injected stream\n",
        "val_recon = autoencoder.predict(val_sequences, verbose=0)\n",
        "val_errors = calculate_reconstruction_errors(val_sequences, val_recon)\n",
        "thresholds = {m: np.mean(v) + 2.5 * np.std(v) for m, v in val_errors.items()}\n",
        "\n",
        "recon = autoencoder.predict(stream, verbose=0)\n",
        "errors = calculate_reconstruction_errors(stream, recon)\n",
        "\n",
        "# combine metric flags using OR only (flag if MSE or MAE exceeds threshold)\n",
        "metric_flags = {m: (errors[m] > thresholds[m]) for m in errors}\n",
        "combined = metric_flags[\"mse\"] | metric_flags[\"mae\"]\n",
        "\n",
        "def flag_consecutive(anomaly_flags, N=2):\n",
        "    result = np.zeros_like(anomaly_flags)\n",
        "    count = 0\n",
        "    for i, f in enumerate(anomaly_flags):\n",
        "        if f:\n",
        "            count += 1\n",
        "            if count >= N:\n",
        "                result[i] = 1\n",
        "        else:\n",
        "            count = 0\n",
        "    return result\n",
        "\n",
        "flags = flag_consecutive(combined.astype(int), N=2)\n",
        "\n",
        "# 4) Table: where each injected segment was detected and % of flagged windows inside it\n",
        "print(f\"\\n{DATASET.upper()} Detection Report\")\n",
        "print(\"-\" * 85)\n",
        "print(f\"{'Attack':20s} | {'Range':>15s} | {'Detection':>20s} | {'% Flagged':>10s}\")\n",
        "print(\"-\" * 85)\n",
        "\n",
        "for (start, end), name in zip(injected_regions, injected_attacks):\n",
        "    segment_flags = flags[start:end]\n",
        "    flagged = int(np.sum(segment_flags))\n",
        "    percent_flagged = (flagged / max(1, (end - start))) * 100\n",
        "    if flagged:\n",
        "        idxs = np.where(segment_flags == 1)[0]\n",
        "        first = start + idxs[0]\n",
        "        last  = start + idxs[-1]\n",
        "        result = f\"Detected: {first} - {last}\"\n",
        "    else:\n",
        "        result = \"Not Detected\"\n",
        "    print(f\"{name:20s} | {start:4d} - {end:<4d}     | {result:>20s} | {percent_flagged:9.1f}%\")\n",
        "print(\"-\" * 85)\n",
        "\n",
        "# 5) Plots + overall metrics\n",
        "plt.figure(figsize=(14, 3), dpi=150)\n",
        "plt.plot(flags, label=\"Detection Flags\", linewidth=1)\n",
        "plt.plot(true_labels, label=\"Ground Truth\", linestyle='--', linewidth=1)\n",
        "plt.title(\"Anomaly Detection - Injected Attack Evaluation\")\n",
        "plt.xlabel(\"Sequence Index\"); plt.ylabel(\"Flag\")\n",
        "plt.legend(); plt.grid(True, alpha=0.4); plt.tight_layout(); plt.show()\n",
        "\n",
        "print(f\"\\n{DATASET.upper()} Classification Report\")\n",
        "print(classification_report(true_labels, flags, target_names=[\"Benign\", \"Attack\"], digits=2))\n",
        "\n",
        "cm = confusion_matrix(true_labels, flags)\n",
        "plt.figure(figsize=(5, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d',\n",
        "            xticklabels=[\"Benign\", \"Attack\"], yticklabels=[\"Benign\", \"Attack\"])\n",
        "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.title(\"Confusion Matrix\")\n",
        "plt.tight_layout(); plt.show()"
      ],
      "metadata": {
        "id": "0ZmsLE5i_ug_"
      },
      "id": "0ZmsLE5i_ug_",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "avendano1daniel (Aug 12, 2025, 9:05:05â€¯PM)",
      "collapsed_sections": [
        "yxRkWXZG6dNq",
        "hTjHgIRO9puT",
        "jFNPPB87q4S9"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}