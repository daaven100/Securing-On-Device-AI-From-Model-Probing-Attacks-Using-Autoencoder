{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Preprocessing"
      ],
      "metadata": {
        "id": "Fj1Gqr_zKtGg"
      },
      "id": "Fj1Gqr_zKtGg"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the original benign dataset locally\n",
        "data = pd.read_csv(\"\")  # <-- Insert path to ImageNet benign dataset\n",
        "\n",
        "# Select features:\n",
        "# Margin loss = Difference between First and Second Logit\n",
        "# SortedList_Second_Element = Second Logit\n",
        "# SortedList_first_Element = First Logit\n",
        "features = ['Margin Loss', 'SortedList_Second_Element', 'SortedList_First_Element']\n",
        "data_selected = data[features]\n",
        "\n",
        "# Quick missing value check\n",
        "print(\"\\nMissing values per feature:\\n\", data_selected.isnull().sum())\n",
        "\n",
        "# Scale only 'Margin Loss'\n",
        "scaler_margin = MinMaxScaler(feature_range=(0, 1))\n",
        "data_selected.loc[:, 'Margin Loss'] = scaler_margin.fit_transform(\n",
        "    data_selected[['Margin Loss']]\n",
        ").ravel().astype('float64')\n",
        "\n",
        "# Keep original logits (no scaling)\n",
        "data_selected['SortedList_Second_Element'] = data['SortedList_Second_Element']\n",
        "data_selected['SortedList_First_Element']  = data['SortedList_First_Element']\n",
        "\n",
        "data_scaled = pd.DataFrame(data_selected, columns=features)\n",
        "\n",
        "# Train/validation split for VAE\n",
        "X_train, X_valid = train_test_split(data_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to NumPy arrays for the model\n",
        "X_train = np.array(X_train).astype(\"float32\")\n",
        "X_valid = np.array(X_valid).astype(\"float32\")\n",
        "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
        "print(f\"Validation set shape: {X_valid.shape}\")\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Margin Loss (original vs scaled)\n",
        "plt.subplot(2, 2, 1)\n",
        "sns.histplot(data['Margin Loss'], kde=True, bins=30, color=\"red\", alpha=0.6)\n",
        "plt.title('Original Margin Loss')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "sns.histplot(data_scaled['Margin Loss'], kde=True, bins=30, color=\"blue\", alpha=0.6)\n",
        "plt.title('Scaled Margin Loss')\n",
        "\n",
        "# Second Logit\n",
        "plt.subplot(2, 2, 3)\n",
        "sns.histplot(data['SortedList_Second_Element'], kde=True, bins=30, color=\"green\", alpha=0.6)\n",
        "plt.title('Second Logit (Original)')\n",
        "\n",
        "# First Logit\n",
        "plt.subplot(2, 2, 4)\n",
        "sns.histplot(data['SortedList_First_Element'], kde=True, bins=30, color=\"orange\", alpha=0.6)\n",
        "plt.title('First Logit (Original)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "lkdOA2OmK_ia"
      },
      "id": "lkdOA2OmK_ia",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: Variational Autoencoder Architecture & Training"
      ],
      "metadata": {
        "id": "bNJ-82fmKv0u"
      },
      "id": "bNJ-82fmKv0u"
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 20  # size of the latent space (z)\n",
        "\n",
        "# Encoder: maps 3 input features -> latent parameters (mean & log-variance)\n",
        "inputs = Input(shape=(3,), name='encoder_input')  # each row has 3 features\n",
        "x = Dense(16)(inputs)\n",
        "x = LeakyReLU(alpha=0.2)(x)\n",
        "x = Dense(8)(x)\n",
        "x = LeakyReLU(alpha=0.2)(x)\n",
        "z_mean = Dense(latent_dim, name='z_mean')(x)       # μ(z)\n",
        "z_log_var = Dense(latent_dim, name='z_log_var')(x) # log σ^2(z)\n",
        "\n",
        "# Reparameterization trick: sample z = μ + σ * ε (keeps backprop valid)\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    batch = K.shape(z_mean)[0]\n",
        "    dim = K.int_shape(z_mean)[1]\n",
        "    epsilon = K.random_normal(shape=(batch, dim))           # ε ~ N(0, I)\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon        # σ = exp(0.5 * log_var)\n",
        "\n",
        "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
        "\n",
        "# Decoder: maps latent z back to 3 features (reconstruction)\n",
        "decoder_input = Input(shape=(latent_dim,), name='decoder_input')\n",
        "decoder_x = Dense(8)(decoder_input)\n",
        "decoder_x = LeakyReLU(alpha=0.2)(decoder_x)\n",
        "decoder_x = Dense(16)(decoder_x)\n",
        "decoder_x = LeakyReLU(alpha=0.2)(decoder_x)\n",
        "outputs = Dense(3, activation='tanh', name='decoder_output')(decoder_x)  # 3 outputs to match inputs\n",
        "\n",
        "decoder = Model(decoder_input, outputs, name='decoder')\n",
        "\n",
        "# Run decoder on sampled z\n",
        "outputs = decoder(z)\n",
        "\n",
        "# Custom VAE Model: adds KL term to the loss (weighted by beta)\n",
        "class VAE(Model):\n",
        "    def __init__(self, encoder, decoder, beta=0.1, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.beta = beta  # weight for KL divergence\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var, z = self.encoder(inputs)\n",
        "        reconstructed = self.decoder(z)\n",
        "\n",
        "        # KL divergence term: encourages posterior to stay near N(0, I)\n",
        "        kl_loss = -0.5 * self.beta * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "        self.add_loss(K.mean(kl_loss))  # add as a regularization loss\n",
        "\n",
        "        return reconstructed\n",
        "\n",
        "# Pack encoder to output (μ, logσ^2, z)\n",
        "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "\n",
        "# Build VAE with a slightly stronger KL weight\n",
        "vae = VAE(encoder, decoder, beta=0.2)\n",
        "\n",
        "# Compile with reconstruction loss (MSE); KL is added via add_loss()\n",
        "vae.compile(optimizer=Adam(learning_rate=0.001), loss=mse)\n",
        "\n",
        "# Ensure arrays are float32 and shaped (num_samples, 3)\n",
        "X_train = np.asarray(X_train).astype('float32').reshape(-1, 3)\n",
        "X_valid = np.asarray(X_valid).astype('float32').reshape(-1, 3)\n",
        "\n",
        "# Stop early if validation loss stops improving\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train VAE to reconstruct inputs\n",
        "history = vae.fit(\n",
        "    X_train, X_train,\n",
        "    epochs=80,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_valid, X_valid),\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Plot training curves (loss = reconstruction + KL)\n",
        "plt.figure(figsize=(8, 5), dpi=300)\n",
        "plt.plot(history.history['loss'], label='Training Loss', linewidth=2.5)\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2.5)\n",
        "plt.title('VAE Training History', fontsize=14, weight='bold')\n",
        "plt.xlabel('Epoch', fontsize=12); plt.ylabel('Loss', fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.6); plt.legend(fontsize=10)\n",
        "plt.tight_layout(); plt.show()\n"
      ],
      "metadata": {
        "id": "ki_Lz_K1RLjS"
      },
      "id": "ki_Lz_K1RLjS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3: Generate Synthetic Samples & Evaluation"
      ],
      "metadata": {
        "id": "eWIMCeeSK2m0"
      },
      "id": "eWIMCeeSK2m0"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate # Synthetic Data\n",
        "num_samples = 25000 # Number of samples Generated\n",
        "latent_samples = np.random.normal(size=(num_samples, latent_dim))\n",
        "synthetic_features = np.clip(decoder.predict(latent_samples), 0, None)\n",
        "\n",
        "# Convert to DataFrame with 3 features\n",
        "synthetic_data = pd.DataFrame(synthetic_features, columns=[\"Margin Loss\", \"SortedList_Second_Element\", \"SortedList_First_Element\"])\n",
        "\n",
        "# Required features\n",
        "features = ['Margin Loss', 'SortedList_Second_Element', 'SortedList_First_Element']\n",
        "data_selected = data[features]\n",
        "\n",
        "# Apply Min-Max Scaling only to 'Margin Loss'\n",
        "scaler_margin = MinMaxScaler(feature_range=(0, 1))\n",
        "data_selected['Margin Loss'] = scaler_margin.fit_transform(data_selected[['Margin Loss']]).ravel().astype('float64')\n",
        "\n",
        "# Keep logits as original values\n",
        "data_selected['SortedList_Second_Element'] = data['SortedList_Second_Element']\n",
        "data_selected['SortedList_First_Element'] = data['SortedList_First_Element']\n",
        "\n",
        "data_scaled = pd.DataFrame(data_selected, columns=features)\n",
        "\n",
        "# Split into training and validation sets\n",
        "X_train, X_valid = train_test_split(data_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to NumPy arrays for TensorFlow\n",
        "X_train = np.array(X_train).astype(\"float32\")\n",
        "X_valid = np.array(X_valid).astype(\"float32\")\n",
        "\n",
        "# Dataset shapes\n",
        "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
        "print(f\"Validation set shape: {X_valid.shape}\")\n",
        "\n",
        "# Plotting area\n",
        "plt.figure(figsize=(18, 10))\n",
        "synthetic_data[\"Margin Loss\"] = synthetic_features[:, 0]\n",
        "synthetic_data[\"SortedList_Second_Element\"] = synthetic_features[:, 1]\n",
        "synthetic_data[\"SortedList_First_Element\"] = synthetic_features[:, 2]\n",
        "\n",
        "# Save synthetic dataset\n",
        "synthetic_data.to_csv(\"Synthetic_Data.csv\", index=False)\n",
        "\n",
        "# Load real data for comparison\n",
        "real_data = pd.read_csv(\"\")[['Margin Loss', 'SortedList_Second_Element', 'SortedList_First_Element']]     # Insert ImageNet Benign file path\n",
        "\n",
        "# Redundancy Analysis\n",
        "num_duplicates = synthetic_data.duplicated().sum()\n",
        "print(f\"Number of duplicate rows in the synthetic dataset: {num_duplicates}\")\n",
        "\n",
        "num_duplicates = real_data.duplicated().sum()\n",
        "print(f\"Number of duplicate rows in the real dataset: {num_duplicates}\")\n",
        "\n",
        "# Count unique values in each feature for synthetic data\n",
        "unique_synthetic_margin = synthetic_data['Margin Loss'].nunique()\n",
        "unique_synthetic_sorted2 = synthetic_data['SortedList_Second_Element'].nunique()\n",
        "unique_synthetic_sorted1 = synthetic_data['SortedList_First_Element'].nunique()\n",
        "\n",
        "# Count unique values in each feature for real data\n",
        "unique_real_margin = real_data['Margin Loss'].nunique()\n",
        "unique_real_sorted2 = real_data['SortedList_Second_Element'].nunique()\n",
        "unique_real_sorted1 = real_data['SortedList_First_Element'].nunique()\n",
        "\n",
        "print(f\"\\nUnique values in 'Margin Loss': Synthetic ({unique_synthetic_margin}) vs Real ({unique_real_margin})\")\n",
        "print(f\"Unique values in 'SortedList_Second_Element': Synthetic ({unique_synthetic_sorted2}) vs Real ({unique_real_sorted2})\")\n",
        "print(f\"Unique values in 'SortedList_First_Element': Synthetic ({unique_synthetic_sorted1}) vs Real ({unique_real_sorted1})\")\n",
        "\n",
        "# Check for overlap between real and synthetic data\n",
        "matching_samples = synthetic_data.merge(real_data, on=['Margin Loss', 'SortedList_Second_Element', 'SortedList_First_Element'], how='inner')\n",
        "num_matching_samples = matching_samples.shape[0]\n",
        "\n",
        "print(f\"\\nNumber of exact matches between synthetic and real data: {num_matching_samples}\")\n",
        "matching_percentage = (num_matching_samples / len(synthetic_data)) * 100\n",
        "print(f\"Percentage of synthetic data that exactly matches real data: {matching_percentage:.2f}%\")\n",
        "\n",
        "# Count total unique values in the original benign dataset\n",
        "unique_real_data_count = real_data.nunique().sum()\n",
        "unique_synthetic_data_count = synthetic_data.nunique().sum()\n",
        "\n",
        "print(f\"\\nTotal unique values in the original benign dataset: {unique_real_data_count}\")\n",
        "print(f\"Total unique values in the synthetic dataset: {unique_synthetic_data_count}\")\n",
        "\n",
        "# Plot comparison of real vs. synthetic distributions\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 4), dpi=300)\n",
        "\n",
        "sns.histplot(real_data['Margin Loss'], kde=True, bins=30, color='blue', label='Real Margin Loss', stat='density', alpha=0.6, ax=axes[0])\n",
        "sns.histplot(synthetic_data['Margin Loss'], kde=True, bins=30, color='red', label='Synthetic Margin Loss', stat='density', alpha=0.6, ax=axes[0])\n",
        "axes[0].set_title('Distribution of Margin Loss')\n",
        "axes[0].legend()\n",
        "\n",
        "sns.histplot(real_data['SortedList_Second_Element'], kde=True, bins=30, color='blue', label='Real SortedList_Second_Element', stat='density', alpha=0.6, ax=axes[1])\n",
        "sns.histplot(synthetic_data['SortedList_Second_Element'], kde=True, bins=30, color='red', label='Synthetic SortedList_Second_Element', stat='density', alpha=0.6, ax=axes[1])\n",
        "axes[1].set_title('Distribution of Second Top Logit')\n",
        "axes[1].legend()\n",
        "\n",
        "sns.histplot(real_data['SortedList_First_Element'], kde=True, bins=30, color='blue', label='Real SortedList_First_Element', stat='density', alpha=0.6, ax=axes[2])\n",
        "sns.histplot(synthetic_data['SortedList_First_Element'], kde=True, bins=30, color='red', label='Synthetic SortedList_First_Element', stat='density', alpha=0.6, ax=axes[2])\n",
        "axes[2].set_title('Distribution of Top Logit')\n",
        "axes[2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compare basic statistics\n",
        "print(\"\\nComparison of Real vs Synthetic Data Statistics:\")\n",
        "print(\"\\nReal Data Statistics:\\n\", real_data.describe())\n",
        "print(\"\\nSynthetic Data Statistics:\\n\", synthetic_data.describe())\n",
        "\n",
        "# Display first few synthetic samples\n",
        "print(\"\\nFirst 5 samples of the synthetic dataset:\")\n",
        "print(synthetic_data.head())"
      ],
      "metadata": {
        "id": "f5yOMYKXRujp"
      },
      "id": "f5yOMYKXRujp",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "avendano1daniel (Aug 13, 2025, 8:21:02 PM)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}